{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import logging\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# print (tf.__version__)\n",
    "# if tf.test.gpu_device_name():\n",
    "#     print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "# else:\n",
    "#     print(\"Please install GPU version of TF\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用层归一layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln(inputs, epsilon=1e-8, scope=\"ln\"):\n",
    "    \"\"\"\n",
    "            使用层归一layer normalization\n",
    "        tensorflow 在实现 Batch Normalization（各个网络层输出的归一化）时，主要用到nn.moments和batch_normalization\n",
    "        其中moments作用是统计矩，mean 是一阶矩，variance 则是二阶中心矩\n",
    "        tf.nn.moments 计算返回的 mean 和 variance 作为 tf.nn.batch_normalization 参数进一步调用\n",
    "        :param inputs: 一个有2个或更多维度的张量，第一个维度是batch_size\n",
    "        :param epsilon: 很小的数值，防止区域划分错误\n",
    "        :param scope: \n",
    "        :return: 返回一个与inputs相同shape和数据的dtype\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        inputs_shape = inputs.get_shape()\n",
    "        params_shape = inputs_shape[-1:]\n",
    "    \n",
    "        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "        beta= tf.get_variable(\"beta\", params_shape, initializer=tf.zeros_initializer())\n",
    "        gamma = tf.get_variable(\"gamma\", params_shape, initializer=tf.ones_initializer())\n",
    "        normalized = (inputs - mean) / ( (variance + epsilon) ** (.5) )\n",
    "        outputs = gamma * normalized + beta\n",
    "        \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask(inputs, queriers=None, keys=None, type=None):\n",
    "    \"\"\"\n",
    "        对Keys或Queries进行遮盖\n",
    "    :param inputs: (N, T_q, T_k)\n",
    "    :param queries: (N, T_q, d)\n",
    "    :param keys: (N, T_k, d)\n",
    "    :return:\n",
    "    \n",
    "    \"\"\"\n",
    "    padding_num = -2 **32 +1\n",
    "    if type in (\"k\",\"key\",\"keys\"):\n",
    "        \"\"\"\n",
    "        代码里 if type in (\"k\", \"key\", \"keys\"):  部分是padding mask，\n",
    "        因为Q乘以V，V的序列后面有很长一部分是全零的向量（这就是我们自定义的padding的对应embedding，我们定义为全0），\n",
    "        因此全零的部分我们让attention的权重为一个很小的值-4.2949673e+09。\n",
    "        \"\"\"\n",
    "        # 生成 masks\n",
    "        masks = tf.sign(tf.reduce_sum(tf.abs(keys), axis=-1))  # 维度：（N， T_k），sign 判断出1,0，,1等值\n",
    "        masks = tf.expand_dims(masks, 1)  # 进行维度的扩展,在下标为1的维度 （N,1,T_k）\n",
    "        masks = tf.tile(masks, [1, tf.shape(queriers)[1], 1]) # 按照列表的形式，进行维度的扩大制定的倍数(N,T_q,t_k)\n",
    "        \n",
    "        # 将 masks 输入到 inputs中去\n",
    "        paddings = tf.ones_like(inputs) * padding_num\n",
    "        # tf.where(input, a, b)，返回的是b，b中元素input = true位置由a中对应位置的元素替代，其他位置元素不变\n",
    "        outputs = tf.where(tf.equal(masks,0),paddings, inputs) #(N,T_q,t_k)\n",
    "    elif type in (\"q\",\"query\",\"queries\"):\n",
    "        \"\"\"\n",
    "        似的，query序列最后面也有可能是一堆padding，不过对queries做padding mask不需要把padding加上一个很小的值，\n",
    "        只要将其置零就行，因为outputs是先key mask，再经过softmax，再进行query mask的。\n",
    "        \"\"\"\n",
    "        # 生成masks\n",
    "        masks = tf.sign(tf.reduce_sum(tf.abs(queriers),axis=1)) # # (N, T_q)\n",
    "        masks = tf.expand_dims(masks, -1)  # (N, T_q, 1)\n",
    "        masks = tf.tile(masks, [1,1, tf.shape(keys)[1]])  # # (N, T_q, T_k)\n",
    "        \n",
    "        outputs = inputs * masks\n",
    "        \n",
    "    elif type in (\"f\",\"future\",\"right\"):\n",
    "        \"\"\"\n",
    "        部分则是我们在做decoder的self attention时要用到的sequence mask，\n",
    "        也就是说在每一步，第i个token关注到的attention只有可能是在第i个单词之前的单词，因为它按理来说，看不到后面的单词。\n",
    "        \"\"\"\n",
    "        diag_vals = tf.ones_like(inputs[0,:,:])  # (T_q, T_k)\n",
    "        tri1 = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense()  # (T_q, T_k)\n",
    "        masks = tf.expand_dims(tri1,0)\n",
    "        masks = tf.tile(masks, [tf.shape[0], 1, 1])  # (N, T_q, T_k)\n",
    "        \n",
    "        paddings = tf.ones_like(masks) * padding_num\n",
    "        outputs = tf.where(tf.equal(masks,0),paddings, inputs)\n",
    "        \n",
    "    else:\n",
    "        print(\"Check if you entered type correctly!\")\n",
    "    \n",
    "    return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Context-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也就是论文里提到的Encoder-Decoder Attention，是两个不同序列之间的attention，与来源于自身的 self-attention 相区别。context-attention有很多，这里使用的是scaled dot-product。通过 query 和 key 的相似性程度来确定 value 的权重分布。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际上这部分代码就是self attention用到的QKV的公式的核心代码，不管是Encoder-Decoder Attention还是Self Attention都是用的这里的scaled dot-product方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q,K,V,causality = False, dropout_rate=0.,\n",
    "                                 training=True,scope=\"scaled_dot_product_attention\"):\n",
    "    \"\"\"\n",
    "    查看原论文中3.2.1attention计算公式：Attention(Q,K,V)=softmax(Q K^T /√dk ) V\n",
    "        :param Q: 查询，三维张量，[N, T_q, d_k].\n",
    "        :param K: keys值，三维张量，[N, T_k, d_v].\n",
    "        :param V: values值，三维张量，[N, T_k, d_v].\n",
    "        :param causality: 布尔值，如果为True，就会对未来的数值进行遮盖\n",
    "        :param dropout_rate: 0到1之间的一个数值\n",
    "        :param training: 布尔值，用来控制dropout\n",
    "        :param scope: \n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
    "        d_k = Q.get_shape().as_list()[-1]\n",
    "        \n",
    "        # dot product\n",
    "        outputs = tf.matmul(Q, tf.transpose(K, [0,2,1]))   #(N, T_q, T_k)\n",
    "        \n",
    "        # scale\n",
    "        outputs /= d_k ** 0.5\n",
    "        \n",
    "        # key masking\n",
    "        outputs = mask(outputs,Q, K, type=\"key\")\n",
    "        \n",
    "        \n",
    "        # 对未来的数值进行遮盖\n",
    "        if causality:\n",
    "            outputs = mask(outputs, type=\"future\")\n",
    "            \n",
    "        # softmax \n",
    "        outputs = tf.nn.softmax(outputs)\n",
    "        attention = tf.transpose(outputs, [0,2,1])\n",
    "        tf.summary.image(\"attention\", tf.expand_dims(attention[:1], -1))\n",
    "        \n",
    "        # query masking\n",
    "        outputs = mask(outputs, Q, K, type=\"query\")\n",
    "        \n",
    "        # dropout\n",
    "        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=training)\n",
    "        \n",
    "        # weighted sum (context vectors), 也是就是输出的最后的 Z\n",
    "        outputs = tf.matmul(outputs, V)   # (N, T_q, d_v)\n",
    "        \n",
    "    return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多头self attention就是Transoformer的核心，就是用上面提到的QKV公式算出分布之后，用h份合在一起来表示，论文中的h为8。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这部分代码主要是先产生QKV向量，然后按照h头来进行划分，然后调用上面的scaled dot-product的方法来计算的。\n",
    "\n",
    "另外这里可以看到代码里将8份self attention分别计算后后concat起来了，然后在self attention层后接了残差连接和layer normalization。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_attention(queries,keys,values,\n",
    "                       num_heads = 8,\n",
    "                       droupout_rate = 0,\n",
    "                       training =True,\n",
    "                       causality=False,\n",
    "                       scope = \"multihead_attention\"):\n",
    "    \"\"\"\n",
    "        查看原论文中3.2.2中multihead_attention构建，\n",
    "        这里是将不同的Queries、Keys和values方式线性地投影h次是有益的。\n",
    "        线性投影分别为dk，dk和dv尺寸。在每个预计版本进行queries、keys、values，\n",
    "        然后并行执行attention功能，产生dv维输出值。这些被连接并再次投影，产生最终值\n",
    "        :param queries: 三维张量[N, T_q, d_model]\n",
    "        :param keys: 三维张量[N, T_k, d_model]\n",
    "        :param values: 三维张量[N, T_k, d_model]\n",
    "        :param num_heads: heads数\n",
    "        :param dropout_rate: \n",
    "        :param training: 控制dropout机制\n",
    "        :param causality: 控制是否遮盖\n",
    "        :param scope: \n",
    "        :return: 三维张量(N, T_q, C) \n",
    "    \n",
    "    \"\"\"\n",
    "    d_model = queries.get_shape().as_list()[-1]\n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        # 线性 拼接？？ dense ：全连接层  相当于添加一个层 \n",
    "        # https://blog.csdn.net/yangfengling1023/article/details/81774580\n",
    "        Q = tf.layers.dense(queries,d_model, use_bias=False) # (N, T_q, d_model)\n",
    "        K = tf.layers.dense(keys, d_model, use_bias=False) # (N, T_k, d_model)\n",
    "        V = tf.layers.dense(values,d_model, use_bias=False) #(N, T_k, d_model)\n",
    "        \n",
    "        # 再进行切分，切分8个？？,再 拼接？？\n",
    "#https://blog.csdn.net/SangrealLilith/article/details/80272346?depth_1-utm_source=distribute.pc_relevant.none-task&utm_source=distribute.pc_relevant.none-task\n",
    "        Q_ = tf.concat(tf.split(Q, num_heads), axis=2)  # (h*N, T_q, d_model/h)\n",
    "        K_ = tf.concat(tf.split(K, num_heads), axis=2) # (h*N, T_k, d_model/h)\n",
    "        V_ = tf.concat(tf.split(V, num_heads), axis=2) #  (h*N, T_k, d_model/h)\n",
    "        \n",
    "        # Attention \n",
    "        outputs = scaled_dot_product_attention(Q_, K_, V_,causality,\n",
    "                                              droupout_rate, training)\n",
    "        \n",
    "        # 重新调整维度 \n",
    "        outputs = tf.concat(tf.split(outputs, num_heads, axis=0),axis=2)  # (N, T_q, d_model)\n",
    "        \n",
    "        # # Residual connection ????  为什么呢\n",
    "        outputs += queries\n",
    "        \n",
    "        # Normalize\n",
    "        outputs = ln(outputs)\n",
    "        \n",
    "    return outputs\n",
    "\n",
    "\n",
    "# 这里提一句，所有的attention都是用scaled dot-product的方法来计算的，\n",
    "# 对于self attention来说，Q=K=V，而对于decoder-encoder attention来说，Q=decoder_input，K=V=memory。\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Positional Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就目前而言，Transformer 架构还没有提取序列顺序的信息，\n",
    "这个信息对于序列而言非常重要，如果缺失了这个信息，可能我们的结果就是：\n",
    "所有词语都对了，但是无法组成有意义的语句。因此模型对序列中的词语出现的位置进行编码。论文中使用的方法是在偶数位置使用正弦编码，在奇数位置使用余弦编码。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(inputs,\n",
    "                       maxlen,\n",
    "                       masking=True,\n",
    "                       scope=\"positional_encoding\"):\n",
    "    \"\"\"\n",
    "    ''\n",
    "        参看论文3.5，由于模型没有循环和卷积，为了让模型知道句子的编号，\n",
    "        就必须加入某些绝对位置信息，来表示token之间的关系。  \n",
    "        positional encoding和embedding有相同的维度，这两个能够相加。\n",
    "        :param inputs: \n",
    "        :param maxlen: \n",
    "        :param masking: \n",
    "        :param scope: \n",
    "        :return: \n",
    "    \"\"\"\n",
    "    E = inputs.get_shape(),as_list()[-1] # static\n",
    "    N, T = tf.shape(inputs)[0], tf.shape(inputs)[1] # dynamic\n",
    "    \n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        # 位置下标\n",
    "        position_ind = tf.tile(tf.expand_dims(tf.range(T),0), [N,1])  # (N, T\n",
    "        # 第一部分的PE函数:sin和cos的参数\n",
    "        position_enc = np.array([\n",
    "            [pos / np.power(10000, (i-i%2)/E) for i in range(E)]\n",
    "            for pos in range(maxlen)\n",
    "        ])\n",
    "        \n",
    "        # 第二部分，将余弦应用于偶数列，将sin应用于奇数。\n",
    "        position_enc[:,0::2] = np.sin(position_enc[:,0::2])  # dim 2i\n",
    "        position_enc[:,1::2] = np.cos(position_enc[:,1::2])  # dim 2i +1\n",
    "        position_enc = tf.convert_to_tensor(position_enc, tf.float32) ## (maxlen, E)\n",
    "        \n",
    "        # lookup\n",
    "        outputs = tf.nn.embedding_lookup(position_enc,position_ind)\n",
    "        \n",
    "        # masks\n",
    "        if masking:\n",
    "            outputs = tf.where(tf.equal(inputs, 0), inputs, outputs)\n",
    "            \n",
    "        \n",
    "        return tf.to_float(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前向网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前向网络是两层全连接层接一个残差连接和layer normalization。　　\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ff(inputs, num_units, scope=\"positionwise_feedforward\"):\n",
    "    \"\"\"\n",
    "    inputs: [N, T, C],\n",
    "    num_units : 两个整数的列表\n",
    "    \n",
    "    返回一个：　和输入维度相同的3d　张量\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        # input layer\n",
    "        outputs = tf.layers.dense(inputs, num_units[0], activation=tf.nn.relu)\n",
    "        \n",
    "        # out layer\n",
    "        outputs = tf.layers.dense(outputs, num_units[1])\n",
    "        \n",
    "        \n",
    "        # Residual connection\n",
    "        outputs += inputs\n",
    "        \n",
    "        # Normalize\n",
    "        outputs = ln(outputs)\n",
    "        \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Smoothing技术"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_smoothing(inputs, epsilon = 0.1):\n",
    "    \"\"\"\n",
    "简单来说就是本来ground truth标签是1的，他改到比如说0.9333，\n",
    "本来是0的，他改到0.0333，这是一个比较经典的平滑技术了。\n",
    "    \n",
    "    \"\"\"\n",
    "    V = inputs.get_shape().as_list()[-1]  # 获取通道的数量\n",
    "    return ((1-epsilon) * inputs) + (epsilon / V)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noam计划衰减学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def noam_scheme(init_lr, global_step, warmup_steps=4000.):\n",
    "    \"\"\"\n",
    "    init_lr: 初始学习率\n",
    "    warmup_steps：　预热步数\n",
    "    global_step：　全局步数，　从１　开始\n",
    "    ｌearning Rate = init_lr * warmup_steps**0.5 * min(global_step * warmup_steps**-1.5, warmup_steps**-0.5)\n",
    "    \"\"\"\n",
    "    \n",
    "    step = tf.cast(global_step + 1, dtype=tf.float32)\n",
    "    \n",
    "    return init_lr * warmup_steps ** 0.5 * tf.minimum(step * warmup_steps ** -1.5, step ** -0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  uitls代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_idx_to_token_tensor(inputs, idx2token):\n",
    "    \"\"\"\n",
    "    将　int32的张量转化成　　字符串张量\n",
    "    \n",
    "    inputs: 1 维的 int32 的张量　索引\n",
    "    idx2token : 字典\n",
    "    返回：　１维的字符串张量\n",
    "    \"\"\"\n",
    "    \n",
    "    def my_func(inputs):\n",
    "        return \" \".join(idx2token[elem] for elem in inputs)\n",
    "    # tf.py_func()接收的是tensor，然后将其转化为numpy array送入我们自定义的my_func函数，\n",
    "    # 最后再将my_func函数输出的numpy array转化为tensor返回。\n",
    "    # https://blog.csdn.net/aaon22357/article/details/82996436\n",
    "    return tf.py_func(my_func, [inputs], tf.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  数据加载方面的代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encode函数用于将字符串转化为数字"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encode函数用于将字符串转化为数字，这里具体方法是输入的是一个字符序列，然后根据空格切分，然后如果是源语言，则每一句话后面加上\"“/s”\"，如果是目标语言，则在每一句话前面加上\"“s”\"，后面加上\"“/s”\"，然后再转化成数字序列。如果是中文，这里很显然要改，具体看是字符级别输入还是词语级别输入."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(inp, type, dict):\n",
    "    \"\"\"\n",
    "    将　１维的　数组\n",
    "    \n",
    "    type: \"x\" 源语言，　ｙ,目标语言\n",
    "    dict:   token2idx 字典\n",
    "    返回：　数字列表\n",
    "    \"\"\"\n",
    "    inp_str = inp.decode(\"utf-8\")\n",
    "    if type == \"x\":\n",
    "        tokens = inp_str.split() + [\"</s>\"]\n",
    "    else:\n",
    "        tokens = [\"<s>\"] + inp_str.split() + [\"</s>\"]\n",
    "        \n",
    "    x = [dict.get(t, dict[\"<unk>\"]) for t in tokens]\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generator_fn方法生成训练和评估集数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码简单讲一下，对于每一个sent1，sent2（源句子，目标句子），sent1经过前面的encode函数转化成x，sent2经过前面的encode函数转化成y之后，decoder的输入decoder_input是y[:-1]，预期输出y是y[1:]，啥意思呢，就是其实是RNN一样的，用来解码输入的前N-1个，期望的输出是从第2个到第N个，也是N-1个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_fn(sents1, sents2, vocab_fpath):\n",
    "    \"\"\"\n",
    "     sents1: 源句子列表\n",
    "    sents2: 目标句子列表\n",
    "    vocab_fpath: s\n",
    "\n",
    "    yields\n",
    "    xs: tuple of\n",
    "        x: list of source token ids in a sent\n",
    "        x_seqlen: int. sequence length of x\n",
    "        sent1: str. raw source (=input) sentence\n",
    "    labels: tuple of\n",
    "        decoder_input: decoder_input: list of encoded decoder inputs\n",
    "        y: list of target token ids in a sent\n",
    "        y_seqlen: int. sequence length of y\n",
    "        sent2: str. target sentence\n",
    "    \"\"\"\n",
    "    token2idx, _ = load_vocab(vocab_fpath)\n",
    "    for sent1, sent2 in zip(sents1, sents2):\n",
    "        x = encode(sent1, \"x\", token2idx)\n",
    "        y = encode(sent2, \"y\", token2idx)\n",
    "        decoder_input, y = y[:-1], y[1:]\n",
    "        \n",
    "        x_seqlen, y_seqlen = len(x), len(y)\n",
    "        \n",
    "        yield (x, x_seqlen, sent1), (decoder_input, y, y_seqlen, sent2)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input_fn方法用来生成Batch数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码其实也比较值得学习，用tf.data.Dataset.from_generator的方式读入数据，不受计算图的影响，比较好。Dataset作为新的API，比以前的feed_dict效率要高一些。关于dataset的简单使用，和一些它代码里用到的API的简单解释\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(sents1, sents2, vocab_fpath, bath_size, shuffle=False):\n",
    "    \"\"\"\n",
    "    成批投放数据\n",
    "    sents1: 源句子列表\n",
    "    sents2: 目标句子列表\n",
    "    vocab_fpath: string. vocabulary file path.\n",
    "    batch_size: scalar 每个批次的大小　张量\n",
    "    shuffle: boolean\n",
    "\n",
    "    Returns\n",
    "    xs: tuple of\n",
    "        x: int32 tensor. (N, T1)\n",
    "        x_seqlens: int32 tensor. (N,)\n",
    "        sents1: str tensor. (N,)\n",
    "    ys: tuple of\n",
    "        decoder_input: int32 tensor. (N, T2)\n",
    "        y: int32 tensor. (N, T2)\n",
    "        y_seqlen: int32 tensor. (N, )\n",
    "        sents2: str tensor. (N,)\n",
    "    \"\"\"\n",
    "    shape = (([None],(),()),\n",
    "            ([None], [None], (),()))\n",
    "    types = ((tf.int32, tf.int32, tf.string),\n",
    "            (tf.int32, tf.int32, tf.int32, tf.string))\n",
    "    \n",
    "    paddings = ((0,0,''),\n",
    "               (0,0,0,''))\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "            generator_fn,output_shapes=shape,\n",
    "            output_types=types,\n",
    "            args=(sents1, sents2, vocab_fpath)) # <- arguments for generator_fn. converted to np string arrays\n",
    "    if shuffle:  # for training\n",
    "        dataset = dataset.shuffle(128 * bath_size)\n",
    "        \n",
    "    dataset = dataset.repeat()  # 一直迭代下去\n",
    "    dataset = dataset.padded_batch(bath_size, shapes, paddings).prefetch(1)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 整合模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    \"\"\"\n",
    "    xs: tuple of\n",
    "        x: int32 tensor. (N, T1)\n",
    "        x_seqlens: int32 tensor. (N,)\n",
    "        sents1: str tensor. (N,)\n",
    "    ys: tuple of\n",
    "        decoder_input: int32 tensor. (N, T2)\n",
    "        y: int32 tensor. (N, T2)\n",
    "        y_seqlen: int32 tensor. (N, )\n",
    "        sents2: str tensor. (N,)\n",
    "    training: boolean\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hp):  # hp 输入向量\n",
    "        self.hp = hp\n",
    "        self.token2idx, self.idx2token = load_vocab(hp.vocab)\n",
    "        self.embeddings = get_token_embeddings(self.hp.vocab_size, self.hp.d_model,zero_pad=True)\n",
    "        \n",
    "    def encode(self, xs, training=True):   # xs 是　input_fn方法用来生成Batch数据　返回的数值\n",
    "        \"\"\"\n",
    "        返回：memory: encoder outputs. (N, T1, d_model)\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"encoder\", reuse=tf.AUTO_REUSE):\n",
    "            x, seqlens, sents1 = xs\n",
    "                \n",
    "            # emedding \n",
    "            enc = tf.nn.embedding_lookup(self.embeddings,x) #(N, T1, d_model)\n",
    "            enc *= self.hp.d_model ** 0.5  # scale\n",
    "            \n",
    "            enc += positional_encoding(enc, self.hp.maxlen1)\n",
    "            enc = tf.layers.dropout(enc, self.hp.dropout_rate, training=training)\n",
    "            \n",
    "            # Blocks\n",
    "            for i in range(self.hp.num_blocks):\n",
    "                with tf.variable_scope(\"num_blocaks_{}\".format(i), reuse=tf.AUTO_REUSE):\n",
    "                    # self-attention\n",
    "                    enc = multihead_attention(queries=enc,\n",
    "                                             keys = enc,\n",
    "                                             values =enc,\n",
    "                                             num_heads=self.hp.num_heads,\n",
    "                                             droupout_rate=self.hp.dropout_rate,\n",
    "                                             training=training,\n",
    "                                             causality=False)\n",
    "                    # feed forward\n",
    "                    enc = ff(enc, num_units=[self.hp.d_ff, self.hp.d_model])\n",
    "                    \n",
    "            memory = enc\n",
    "            return memory, sents1\n",
    "        \n",
    "    def decode(self, ys, memory, training=True):\n",
    "        \"\"\"\n",
    "        memory: encoder outputs, (N, T1, d_model)\n",
    "\n",
    "        Returns\n",
    "        logits: (N, T2, V). float32.\n",
    "        y_hat: (N, T2). int32\n",
    "        y: (N, T2). int32\n",
    "        sents2: (N,). string.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"decoder\",reuse=tf.AUTO_REUSE):\n",
    "            decode_inputs, y, seqlens, sents2 = ys   #  也是　input_fn方法用来生成Batch数据　返回的数值\n",
    "\n",
    "            # embedding \n",
    "            dec = tf.nn.embedding_lookup(self.embeddings, decode_inputs)  # (N, T2, d_model)\n",
    "            dec *= self.hp.d_model ** 0.5 # 标量\n",
    "            dec += positional_encoding(dec, self.hp.maxlen2)\n",
    "            dec = tf.layers.dropout(dec, self.hp.dropout_rate, training=training)\n",
    "\n",
    "            # Blocks\n",
    "            for i in range(self.hp.num_blocks):\n",
    "                with tf.variable_scope(\"num_blocks_{}\".format(i), reuse=tf.AUTO_REUSE):\n",
    "                 # mask self-attention (请注意，因果关系此时为True)\n",
    "                    dec = multihead_attention(queries=dec,\n",
    "                                             keys=dec,\n",
    "                                             values=dec,\n",
    "                                             num_heads=self.hp.num_heads,\n",
    "                                             droupout_rate=self.hp.dropout_rate,\n",
    "                                             training=training,\n",
    "                                             causality=True,\n",
    "                                             scope=\"self_attention\")\n",
    "\n",
    "                # 普通的attention\n",
    "                    dec = multihead_attention(queries=dec,\n",
    "                         keys=dec,\n",
    "                         values=dec,\n",
    "                         num_heads=self.hp.num_heads,\n",
    "                         droupout_rate=self.hp.dropout_rate,\n",
    "                         training=training,\n",
    "                         causality=False,\n",
    "                         scope=\"vanilla_attention\")\n",
    "\n",
    "                    ## feed Forward\n",
    "                    dec = ff(dec, num_units=[self.hp.d_ff, self.hp.d_model])\n",
    "\n",
    "        # 最后的线性连接层，　embedding 权重是共享的\n",
    "        weights = tf.transpose(self.embeddings) # (d_model, vocab_size)\n",
    "# https://blog.csdn.net/weixin_39274659/article/details/87869527?depth_1-utm_source=distribute.pc_relevant.none-task&utm_source=distribute.pc_relevant.none-task\n",
    "       #  tf.einsum 的详细使用方法介绍 \n",
    "        logits = tf.einsum(\"ntd,dk->ntk\",dec, weights)  #(N, T2, vocab_size)\n",
    "        y_hat = tf.to_int32(tf.argmax(logits, axis=-1))\n",
    "\n",
    "        return logits, y_hat, y,sents2\n",
    "        \n",
    "        \n",
    "        \n",
    "    def train(self, xs, ys):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        loss: scalar.\n",
    "        train_op: training operation\n",
    "        global_step: scalar.\n",
    "        summaries: training summary node\n",
    "        \n",
    "        \"\"\"\n",
    "        # forward\n",
    "        memory, sents1 = self.encode(xs)\n",
    "        \n",
    "        logits, preds, y, sents2 = self.decode(ys, memory)\n",
    "        \n",
    "        # train scheme\n",
    "        y_ = label_smoothing(tf.one_hot(y, depth=self.hp.vocab_size))\n",
    "        ce = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_)\n",
    "        nonpadding = tf.to_float(tf.not_equal(y,self.token2idx[\"<pad>\"]))  # 0: <pad>\n",
    "        #  测试一下********************************************\n",
    "        print(tf.reduce_sum(nonpadding))\n",
    "        # ***************************************************\n",
    "        loss = tf.reduce_sum(ce * nonpadding) / (tf.reduce_sum(nonpadding) + 1e-7)\n",
    "        \n",
    "        global_step = tf.train.get_or_create_global_step()\n",
    "        # 这里用了一个Noam计划衰减学习率\n",
    "        lr = noam_scheme(self.hp.lr, global_step, self.hp.warmup_steps)\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "        \n",
    "        \n",
    "        tf.summary.scalar(\"lr\",lr)\n",
    "        tf.summary.scalar(\"loss\",loss)\n",
    "        tf.summary.scalar(\"global_step\",global_step)\n",
    "        \n",
    "        \n",
    "        summaries = tf.summary.merge_all()\n",
    "        \n",
    "        return loss, train_op, global_step, summaries\n",
    "    \n",
    "    def enval(self, xs, ys):\n",
    "        \"\"\"\n",
    "        自我预测\n",
    "        忽略输入y\n",
    "        Returns\n",
    "        y_hat: (N, T2)\n",
    "        \"\"\"\n",
    "        decoder_inputs, y, y_seqlen, sents = ys\n",
    "        \n",
    "        decoder_inputs = tf.ones((tf.shape(xs[0])[0],1),tf.int32) * self.token2idx[\"<s>\"]\n",
    "        ys = (decode, y, y_seqlen, sents2)\n",
    "        \n",
    "        memory, sents1 = self.encode(xs, False)\n",
    "        logging.info(\"Inference graph is being built. Please be patient.\")\n",
    "        for _ in tqdm(range(self.hp.maxlen2)):\n",
    "            logits, y_hat, y, sents2 = self.decode(ys, memory, False)\n",
    "            if tf.reduce_sum(y_hat,1) == self.token2idx[\"<pad>\"]:\n",
    "                break\n",
    "            \n",
    "            _decoder_inputs = tf.concat((decoder_inputs, y_hat),1)\n",
    "            ys = (_decoder_inputs, y, y_seqlen, sents2)\n",
    "            \n",
    "        #监测随机样本\n",
    "        n = tf.random_uniform((), 0, tf.shape(y_hat)[0]-1, tf.int32)\n",
    "        sent1 = sents1[n]\n",
    "        pred = convert_idx_to_token_tensor(y_hat[n], self.idx2token)\n",
    "        sent2 =sents2[n]\n",
    "        \n",
    "        tf.summary.text(\"sent1\",sent1)\n",
    "        tf.summary.text(\"pred\",pred)\n",
    "        rf.summary.text(\"sent2\",sent2)\n",
    "        summaries = tf.summary.merge_all()\n",
    "        \n",
    "        return y_hat, summaries\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "your_env_name",
   "language": "python",
   "name": "your_env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
